{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/111Vidya/PySpark-Fundamentals/blob/main/Apr_24_Data_Transformations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUAUA8-lA48V"
      },
      "source": [
        "# Data Transformations\n",
        "\n",
        "You won't always get data in a convienent format, often you will have to deal with data that is non-numerical, such as customer names, or zipcodes, country names, etc...\n",
        "\n",
        "A big part of working with data is using your own domain knowledge to build an intuition of how to deal with the data, sometimes the best course of action is to drop the data, other times feature-engineering is a good way to go, or you could try to transform the data into something the Machine Learning Algorithms will understand.\n",
        "\n",
        "Spark has several built in methods of dealing with thse transformations, check them all out here: http://spark.apache.org/docs/latest/ml-features.html\n",
        "\n",
        "Let's see some examples of all of this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04Wn2dFpA48X",
        "outputId": "47b4ef0e-89df-4159-ec6f-ea21505a9b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=095c414a20295e40db2174ee85698f0ba66f6edbbbf94d2f649b69cd2c7c5d64\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iIk9MvhnA48Y"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName('data').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "phY_v-AuA48Y"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('fake_customers.csv',inferSchema=True,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kNRavm1A48Y",
        "outputId": "091b72a8-cffe-41cd-d488-0e1ff7e5895a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+-----+\n",
            "|   Name|     Phone|Group|\n",
            "+-------+----------+-----+\n",
            "|   John|4085552424|    A|\n",
            "|   Mike|3105552738|    B|\n",
            "| Cassie|4085552424|    B|\n",
            "|  Laura|3105552438|    B|\n",
            "|  Sarah|4085551234|    A|\n",
            "|  David|3105557463|    C|\n",
            "|   Zach|4085553987|    C|\n",
            "|  Kiera|3105552938|    A|\n",
            "|  Alexa|4085559467|    C|\n",
            "|Karissa|3105553475|    A|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ZmiPU_A48Z"
      },
      "source": [
        "## Data Transformation\n",
        "##Data Features\n",
        "\n",
        "### StringIndexer\n",
        "\n",
        "We often have to convert string information into numerical information as a categorical feature. This is easily done with the StringIndexer Method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXw4yiM-A48Z",
        "outputId": "cba10f4a-1c1b-4505-c49c-11b4e34a09ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+-------------+\n",
            "|user_id|category|categoryIndex|\n",
            "+-------+--------+-------------+\n",
            "|      0|       a|          0.0|\n",
            "|      1|       b|          2.0|\n",
            "|      2|       c|          1.0|\n",
            "|      3|       a|          0.0|\n",
            "|      4|       a|          0.0|\n",
            "|      5|       c|          1.0|\n",
            "+-------+--------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# convert string to numeric and assign categorical value\n",
        "from pyspark.ml.feature import StringIndexer # Library for feature transformation\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
        "    [\"user_id\", \"category\"])\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
        "indexed = indexer.fit(df).transform(df)\n",
        "indexed.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvodptMrA48Z"
      },
      "source": [
        "The next step would be to encode these categories into \"dummy\" variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AU9gcn_nA48a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcoGcBgLA48a"
      },
      "source": [
        "### VectorIndexer\n",
        "\n",
        "VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.\n",
        "\n",
        "Assume that we have a DataFrame with the columns id, hour, mobile, userFeatures, and clicked:\n",
        "\n",
        "     id | hour | mobile | userFeatures     | clicked\n",
        "    ----|------|--------|------------------|---------\n",
        "     0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0\n",
        "     \n",
        "userFeatures is a vector column that contains three user features. We want to combine hour, mobile, and userFeatures into a single feature vector called features and use it to predict clicked or not. If we set VectorAssembler’s input columns to hour, mobile, and userFeatures and output column to features, after transformation we should get the following DataFrame:\n",
        "\n",
        "     id | hour | mobile | userFeatures     | clicked | features\n",
        "    ----|------|--------|------------------|---------|-----------------------------\n",
        "     0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0     | [18.0, 1.0, 0.0, 10.0, 0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu1zJI49A48b",
        "outputId": "b2f6faa5-1efe-4ce7-957f-553e679c927e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+--------------+-------+\n",
            "| id|hour|mobile|  userFeatures|clicked|\n",
            "+---+----+------+--------------+-------+\n",
            "|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|\n",
            "+---+----+------+--------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.linalg import Vectors # libraries for vector transformation .linearalgebra\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "dataset = spark.createDataFrame(\n",
        "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
        "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
        "dataset.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EEmc5fOA48b",
        "outputId": "34f2991f-0034-46f7-8e09-3c1d7ac568c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n",
            "+--------------------+-------+\n",
            "|            features|clicked|\n",
            "+--------------------+-------+\n",
            "|[18.0,1.0,0.0,10....|    1.0|\n",
            "+--------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
        "    outputCol=\"features\")\n",
        "\n",
        "output = assembler.transform(dataset)\n",
        "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
        "output.select(\"features\", \"clicked\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr43d9Z3A48c"
      },
      "source": [
        "There ar emany more data transformations available, we will cover them once we encounter a need for them, for now these were the most important ones.\n",
        "\n",
        "Let's continue on to Linear Regression!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uHl8ViFdA48c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}